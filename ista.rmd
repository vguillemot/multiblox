---
title: "ISTA pour la régression multiple"
author: "Vincent Guillemot et Cathy Philippe"
date: "25/02/2015"
output: pdf_document
header-includes: \newcommand{\prox}{\text{prox}}
---

Le problème de la régression multiple  de $X$ sur $y$ avec une pénalité $\ell_1$ se formule de la manière suivante :

\[
  \min_{\beta} \{ \frac{1}{2} \|y - X\beta \|^2 + \alpha \|\beta\|_1 = R(\beta) + \alpha \|\beta\|_1 \}
\]

LASSO est une méthode classique qui permet d'obtenir la solution du problème précédent. Nous voudrions dans ce rapport explorer une méthode alternative : ISTA[^1].

[^1]: Cf. par exemple ce [cours-ci.](http://www.seas.ucla.edu/~vandenbe/236C/lectures/proxgrad.pdf)

Une version accélérée de ISTA est appelée FISTA[^2].

[^2]: Cf. par exemple ce [cours-là.](http://www.seas.ucla.edu/~vandenbe/236C/lectures/fgrad.pdf)

# Principe

Le principe de cette méthode est de calculer à chaque itération la mise à jour du vecteur des poids de la manière suivante :
\[
  \beta^{k+1} \leftarrow \prox_{t \lambda \|\cdot\|_1} \left(  \beta^k - t \nabla_R (\beta^k) \right),
\]
avec

* $\nabla_R (\beta) = X^\top (X\beta - y)$,
* $t = \left( \lambda_{\max} (X^\top X) \right)^{-1}$,
* $\prox_{t \alpha \|\cdot\|_1}$ est l'opérateur proximal de la norm $\ell_1$ multiplié par $t \lambda$.

# Implémentation en R

Nous proposons d'implémenter en R cet algorithme. Il faut tout d'abord déclarer les fonctions permettant de calculer le gradient de la fonction de perte et l'opérateur proximal.

```{r definitions}
grad <- function(X,y,beta) t(X) %*% (X %*%beta - y)
prox <- function(beta,t,alpha) ifelse(abs(beta)<t*alpha,0,beta-t*alpha*sign(beta))
```

Ces fonctions étant définies, nous pouvons implémenter l'algorithme ISTA.

```{r ista}
ista <- function(X, y, alpha, kmax=10000, epsilon=1e-10) {
  p <- ncol(X)
  n <- nrow(X)
  betaold <- rnorm(p)
  t <- 1/max(eigen(t(X)%*%X)$values)
 
  for (k in 1:kmax) {
    betanew <- prox(betaold - t*grad(X,y,betaold),t,alpha)
    if ( sum((betanew-betaold)**2) < epsilon ) break
    betaold <- betanew
  }
 
  return(list(beta=betanew, k=k))
}
```

# Test sur données simulées

Les données simulées :

```{r}
n <- 50
p <- 100
set.seed(1234)

betastar <- c(1, 3, -1, 5, rep(0, p-4))
X <- matrix(rnorm(n*p), n, p)

y <- X %*% betastar + 0.1*rnorm(n)
```

Le test avec ISTA :

```{r}
system.time(res1 <- ista(X, y, alpha=1))
```


```{r}
norml1 <- function(beta)  sum(abs(beta))
R <- function(X,y,beta) sum( (X%*%beta - y)**2 )
```

# ISTA avec un pas adaptatif, calculé par line search

Pour le pas $t$ constant et $0 \le t \le \frac{1}{L}$, $L$ étant la constante de Lipschitz, l'inégalité suivante est vérifiée :
\[
  g(x - tG_t(x)) \le g(x) - t \nabla g(x)^\top G_t(x) + \frac{t}{2}\Vert G_t(x)\Vert_2^2  (1)
\]
où :
\[
  G_t(x) = \frac{1}{t}\left( x - \prox_{th}(x - t \nabla g(x)) \right)
\]
est le pas négatif dans la mise à jour du gradient proximal.

Si $L$ n'est pas connu, on peut satisfaire l'inégalité (1) par *backtracking line search* : on commence à un pas $\hat{t} > 0$ et on diminue sa valeur, $\hat{t} \leftarrow \beta\hat{t}$ avec $0 < \beta < 1$, jusqu'à ce que (1) soit satisfaite.

```{r}
neg_step <- function(X, y, t, beta, alpha){
  return((1/t) * (beta - prox(beta - t * grad(X, y, beta), t, alpha)))
}
line_search_crit <- function(X, y, beta, t, alpha){
  return(ifelse(R(X, y, beta - t*neg_step(X, y, t, beta, alpha)) <= R(X, y, beta) - t*t(grad(X,y,beta)) %*% neg_step(X, y, t, beta, alpha) + (t/2)*sum(neg_step(X, y, t, beta, alpha))**2, TRUE, FALSE)) 
}
step_line_search <- function(X, y, beta, t=1, tau=0.5, alpha){
  while(!line_search_crit(X, y, beta, t, alpha)){
    t <- tau*t
  }
  return(t)
}
```

où $\alpha$ est le paramètre de sparsité et $t$ le pas à calculer à chaque étape de ISTA.

```{r}
ista_adapt <- function(X, y, alpha, kmax=10000, epsilon=1e-10) {
  p <- ncol(X)
  n <- nrow(X)
  betaold <- rnorm(p)
  t <- 1/max(eigen(t(X)%*%X)$values)
 
  for (k in 1:kmax) {
    t <- step_line_search(X, y, betaold, t, tau=0.1, alpha)
    betanew <- prox(betaold - t*grad(X,y,betaold), t, alpha)
    if ( sum((betanew-betaold)**2) < epsilon ) break
    betaold <- betanew
  }
 
  return(list(beta=betanew, k=k))
}
```

## Test sur données simulées
```{r}
system.time(res1 <- ista(X, y, alpha=1))
system.time(res2 <- ista_adapt(X, y, alpha=1))
(res1$k)
(res2$k)
(res<-cbind(res_ista=res1$beta, res_ista_adapt = res2$beta, beta = betastar))
```
