---
title: "ISTA pour la régression multiple"
author: "Vincent Guillemot et Cathy Philippe"
date: "25/02/2015"
output: pdf_document
header-includes: \newcommand{\prox}{\text{prox}}
---

Le problème de la régression multiple  de $X$ sur $y$ avec une pénalité $\ell_1$ se formule de la manière suivante :

\[
  \min_{\beta} \{ \frac{1}{2} \|y - X\beta \|^2 + \alpha \|\beta\|_1 = R(\beta) + \alpha \|\beta\|_1 \}
\]

LASSO est une méthode classique qui permet d'obtenir la solution du problème précédent. Nous voudrions dans ce rapport explorer une méthode alternative : ISTA[^1].

[^1]: Cf. par exemple ce [cours-ci.](http://www.seas.ucla.edu/~vandenbe/236C/lectures/proxgrad.pdf)

Une version accélérée de ISTA est appelée FISTA[^2].

[^2]: Cf. par exemple ce [cours-là.](http://www.seas.ucla.edu/~vandenbe/236C/lectures/fgrad.pdf)

# Principe

Le principe de cette méthode est de calculer à chaque itération la mise à jour du vecteur des poids de la manière suivante :
\[
  \beta^{k+1} \leftarrow \prox_{t \lambda \|\cdot\|_1} \left(  \beta^k - t \nabla_R (\beta^k) \right),
\]
avec

* $\nabla_R (\beta) = X^\top (X\beta - y)$,
* $t = \left( \lambda_{\max} (X^\top X) \right)^{-1}$,
* $\prox_{t \alpha \|\cdot\|_1}$ est l'opérateur proximal de la norm $\ell_1$ multiplié par $t \lambda$.

# Implémentation en R

Nous proposons d'implémenter en R cet algorithme. Il faut tout d'abord déclarer les fonctions permettant de calculer le gradient de la fonction de perte et l'opérateur proximal.

```{r definitions}
grad <- function(X,y,beta) t(X) %*% (X %*%beta - y)
prox <- function(beta,t,alpha) ifelse(abs(beta)<t*alpha,0,beta-t*alpha*sign(beta))
```

Ces fonctions étant définies, nous pouvons implémenter l'algorithme ISTA.

```{r ista}
ista <- function(X, y, alpha, kmax=10000, epsilon=1e-10) {
  p <- ncol(X)
  n <- nrow(X)
  betaold <- rnorm(p)
  t <- 1/max(eigen(t(X)%*%X)$values)
 
  for (k in 1:kmax) {
    betanew <- prox(betaold - t*grad(X,y,betaold),t,alpha)
    if ( sum((betanew-betaold)**2) < epsilon ) break
    betaold <- betanew
  }
 
  return(list(beta=betanew, k=k))
}
```

# Test sur données simulées

Les données simulées :

```{r}
n <- 50
p <- 20
set.seed(1234)

betastar <- c(1, 3, -1, 5, rep(0, p-4))
X <- matrix(rnorm(n*p), n, p)

y <- X %*% betastar + 0.1*rnorm(n)
```

Le test avec ISTA :

```{r}
system.time(res1 <- ista(X, y, alpha=1))
```


```{r}
norml1 <- function(beta)  sum(abs(beta))
R <- function(X,y,beta) sum( (X%*%beta - y)**2 )
```

# ISTA avec un pas adaptatif, calculé par line search

Pour le pas $t$ constant et $0 \le t \le \frac{1}{L}$, $L$ étant la constante de Lipschitz, l'inégalité suivante est vérifiée :
\[
  g(x - tG_t(x)) \le g(x) - t \nabla g(x)^\top G_t(x) + \frac{t}{2}\Vert G_t(x)\Vert_2^2  (1)
\]
où :
\[
  G_t(x) = \frac{1}{t}\left( x - \prox_{th}(x - t \nabla g(x)) \right)
\]
est le pas négatif dans la mise à jour du gradient proximal.

```{r negstep}
neg_step <- function(X, y, t, beta, alpha, g){
  return((1/t) * (beta - prox(beta - t * g, t, alpha)))
}
```

Si $L$ n'est pas connu, on peut satisfaire l'inégalité (1) par *backtracking line search* : on commence à un pas $\hat{t} > 0$ et on diminue sa valeur, $\hat{t} \leftarrow \beta\hat{t}$ avec $0 < \beta < 1$, jusqu'à ce que (1) soit satisfaite.

```{r}
step_line_search <- function(X, y, beta, t=10, tau=0.5, alpha, kmax=1000){
  R <- R(X, y, beta)
  g <- grad(X,y,beta)
  Gt <- neg_step(X, y, t, beta, alpha, g)
  Rt <- R(X, y, beta - t*Gt)
  for (k in 1:kmax) {
    if (Rt <= R -t*t(g) %*% Gt + t/2*sum(Gt**2)) break
    t <- tau*t
    Gt <- neg_step(X, y, t, beta, alpha, g)
    Rt <- R(X, y, beta - t*Gt)
  }
  return(t)
}
```

où $\alpha$ est le paramètre de sparsité et $t$ le pas à calculer à chaque étape de ISTA.

```{r}
ista_adapt <- function(X, y, alpha, kmax=10000, epsilon=1e-10) {
  p <- ncol(X)
  n <- nrow(X)
  betaold <- rnorm(p)
  t <- 1/max(eigen(t(X)%*%X)$values)
 
  for (k in 1:kmax) {
    t <- step_line_search(X, y, betaold, t, tau=0.95, alpha)
    betanew <- prox(betaold - t*grad(X,y,betaold), t, alpha)
    if ( sum((betanew-betaold)**2) < epsilon ) break
    betaold <- betanew
  }
 
  return(list(beta=betanew, k=k))
}
```

## Test sur données simulées

Le test sur les mêmes données simulées que précedemment permet de comprendre l'intérêt que l'on a à connaître la valeur de la constante de Lipschitz $L$: cela permet d'économiser du temps de calcul...

```{r comparaison, echo=FALSE, results='asis'}
t1 <- system.time(res1 <- ista(X, y, alpha=1))
t2 <- system.time(res2 <- ista_adapt(X, y, alpha=1))
k1 <- res1$k
k2 <- res2$k

knitr::kable( data.frame("ISTA" = c(t1[3], k1), "ISTA adaptatif" = c(t2[3], k2),
                         row.names=c("Temps","Nbre d'itérations"), check.names = F))

```

Les valeurs des coefficients obtenus sont très proches :

```{r tab_coefs, echo=FALSE, results='asis'}
knitr::kable(data.frame("ISTA" = res1$beta, "ISTA adaptatif" = res2$beta, 
                        "$\\beta^*$" = betastar,
                        check.names = F))
```

