---
title: "istacox - test fonctions framework MapRedR"
author: "Cathy PHILIPPE"
date: "September 14, 2015"
output: pdf_document
header-includes: 
  \newcommand{\prox}{\text{prox}}
  \usepackage{graphicx}
---
Les fonctions ont été modifiées pour etre insérées dans le framework MapRedR. Notamment, il n'y a plus qu'une seule fonction qui met en oeuvre ISTA ou FISTA, avec un pas adaptatif ou non. Le choix se fait grace aux parametres.

Nous voulons les tester à nouveau sur données simulées avant de les placer dans le framework MapRedR.
```{r}

grad <- function(X, beta, I, R, gamma) {
  wij <- mapply( function(i, j) t(exp( X[j, ]%*%beta) / sum( exp(X[R[[sprintf("R%i", i)]], ]%*%beta))), I, R)
  names(wij) <- names(R)
  xbar <- t(sapply(names(R), function(r) wij[[r]]%*%X[R[[r]], ]) )
  grad <- -colSums( X[I, ] - xbar ) + gamma*beta 
}

norm.l1 <- function(beta){
  return(sum(abs(beta)))
}

norm.l2.2 <- function(beta){
  return(sum(beta^2))
}

```

# ISTACOX avec un pas adaptatif, calculé par line search

Pour le pas $t$ constant et $0 \le t \le \frac{1}{L}$, $L$ étant la constante de Lipschitz, l'inégalité suivante est vérifiée :
\[
  g(x - tG_t(x)) \le g(x) - t \nabla g(x)^\top G_t(x) + \frac{t}{2}\Vert G_t(x)\Vert_2^2  (1)
\]
où :
\[
  G_t(x) = \frac{1}{t}\left( x - \prox_{th}(x - t \nabla g(x)) \right)
\]
est le pas négatif dans la mise à jour du gradient proximal.

```{r}
neg_step <- function(X, t, beta, alpha, g){
  return((1/t) * (beta - prox(beta - t * g, t, alpha)))
}
```

Si $L$ n'est pas connu, on peut satisfaire l'inégalité (1) par *backtracking line search* : on commence à un pas $\hat{t} > 0$ et on diminue sa valeur, $\hat{t} \leftarrow \beta\hat{t}$ avec $0 < \beta < 1$, jusqu'à ce que (1) soit satisfaite.
Il nous faut la fonction de perte $R_{\ell}$ que nous n'avions pas jusqu'alors.

```{r}
R_ell <- function(X, beta, I, R, gamma){
  truc <- mapply( function(i, j) {X[j, ]%*%beta -log(sum( exp(X[R[[sprintf("R%i", i)]], ]%*%beta)))}, I, R, SIMPLIFY = F)
  - sum(unlist(truc)) + gamma/2 * norm.l2.2(beta)
}


prox <- function(beta,t,alpha) ifelse(abs(beta)<t*alpha,0,beta-t*alpha*sign(beta))

istacox_step_line_search <- function(X, beta, I, R, t=10, tau=0.5, alpha, gamma, kmax=1000){
  R_l <- R_ell(X, beta, I, R, gamma)
  g <- grad(X, beta, I, R, gamma)
  Gt <- neg_step(X, t, beta, alpha, g)
  Rt_l <- R_ell(X, beta - t*Gt, I, R, gamma)

  for (k in 1:kmax) {
    if (Rt_l <= R_l -t*t(g) %*% Gt + t/2*sum(Gt**2)) break
    t <- tau*t
    Gt <- neg_step(X, t, beta, alpha, g)
    Rt_l <- R_ell(X, beta - t*Gt, I, R, gamma)
  }
  return(t)
}
```

où $\alpha$ est le paramètre de sparsité, $\gamma$ celui de la ridge et $t$ le pas à calculer à chaque étape de ISTACOX.

```{r}
fistacox_step_line_search <- function(X, u, I, R, t=10, tau=0.95, alpha, gamma, kmax=1000){
  Ru_l <- R_ell(X, u, I, R, gamma)
  gradu <- grad(X, u, I, R, gamma)
  x <- prox(u - t*gradu, t, alpha)
  Rx_l <- R_ell(X, x, I, R, gamma)

  for (k in 1:kmax) {
    if (Rx_l <= Ru_l +  crossprod(gradu, x-u) + 1/(2*t)*sum((x-u)**2)) break
    t <- tau*t
    x <- prox(u - t*gradu,t,alpha)
    Rx_l <- R_ell(X, x, I, R, gamma)
  }
  return(t)
}

istacox <- function(X, I, R, alpha, gamma, kmax=1000, epsilon=1e-10, 
                    fast=FALSE, ada=FALSE) {
  p <- ncol(X)
  n <- nrow(X)
  betaold <- rnorm(p)
  t <- 1/max(eigen(t(X)%*%X)$values)
  
  betanew <- prox(betaold - t*grad(X, betaold, I, R, gamma),t,alpha)
  
  for (k in 2:kmax) {
    if (fast) {
      u <- betanew + (k-1) / (k+2) * (betanew - betaold)
    } else {
      u <- betanew
    }
    if (ada) {
      if (fast) {
        t <- fistacox_step_line_search(X, u, I, R, t, tau=0.95, alpha, gamma)
      } else {
        t <- istacox_step_line_search(X, betaold, I, R, t, tau=0.95, alpha, gamma)
      }
    }
    betaold <- betanew
    betanew <- prox(u - t*grad(X, u, I, R, gamma), t, alpha)
    #print(betaold)
    #print(betanew)
    if (sum((betanew-betaold)**2) < epsilon ) break
  }
  return(list(beta=betanew, k=k))
}
```

# Test sur données simulées

Le script générant des données simulées présenté dans le cadre suivant est extrait de l'aide de la fonction `glmnet::glmnet` :

```{r coxsim, echo=TRUE}
#Cox
set.seed(10101)
# N <- 50 ; p <- 102
N <- 250 ; p <- 66
nzc <- p/3
X <- matrix(rnorm(N*p),N,p)
betastar <- rnorm(nzc)
fx <- X[,seq(nzc)]%*%betastar/3
hx <- exp(fx)
ty <- rexp(N,hx)
# censoring indicator
tcens <- rbinom(n=N, prob=.3, size=1)
# y=Surv(ty,1-tcens) with library(survival)
y <- cbind(time=ty,status=1-tcens) 
```

L'application de la méthode `coxnet` à ce jeu de données simulées donne le résultat suivant (notamment sous forme graphique) :

```{r glmnet, fig.align="center", fig.cap="Graphe des coefficients en fonction du paramètre $\\alpha$."}
require(glmnet)
fit <- glmnet(X, y, family="cox")
plot(fit)
```

Avant de pouvoir appliquer ISTA, il faut ordonner les individus, calculer l'ensemble des patients censurés `I` et les ensembles de patients à risque `R`.

```{r i_et_r}
# Ordering data
X.o <- X[order(y[, 1]), ]
y.o <- as.data.frame(y[order(y[, 1]), ])

# uncensored patients
I <- which(y.o$status==1)
# patients at risk
R <- lapply( which(y.o$status==1) , function(i) which( y.o$time >= y.o$time[i] ) )
names(R) <- paste0("R", which(y.o$status==1))
```

Cela fait, on applique ISTA sur les données ordonnées.

```{r}
beta_star <- c(betastar/3, rep(0, p-nzc))
system.time(res <- istacox(X=X.o, I, R, alpha=10, gamma=10, kmax=1000, epsilon=1e-10, 
                    fast=FALSE, ada=FALSE) )
(res$k)
knitr::kable(df <- data.frame("ISTA COX" = res$beta, "$\\beta^*$" = beta_star, check.names=F))
```

## Comparaison des différentes méthodes

Le test sur les mêmes données simulées que précedemment permet de comprendre l'intérêt que l'on a à connaître la valeur de la constante de Lipschitz $L$: cela permet d'économiser du temps de calcul...

```{r comparaison, echo=FALSE, results='asis'}
t0 <- system.time(coxnet_res <- cv.glmnet(x = X, y = y, type.measure = "deviance", family = "cox", alpha=0.5))
(lambda <- coxnet_res$lambda.min/10)
t1 <- system.time(res1 <- istacox(X.o, I, R, alpha=0.5*lambda, gamma=0.25 * lambda, kmax=1000, fast=FALSE, ada=FALSE))
t2 <- system.time(res2 <- istacox(X.o, I, R, alpha=0.5*lambda, gamma=0.25 * lambda, kmax=1000, fast=FALSE, ada=TRUE))
t3 <- system.time(res3 <- istacox(X.o, I, R, alpha=0.5*lambda, gamma=0.25 * lambda, kmax=1000, fast=TRUE, ada=FALSE))
t4 <- system.time(res4 <- istacox(X.o, I, R, alpha=0.5*lambda, gamma=0.25 * lambda, kmax=1000, fast=TRUE, ada=TRUE))

k1 <- res1$k
k2 <- res2$k
k3 <- res3$k
k4 <- res4$k

knitr::kable( data.frame("COXNET" = c(t0[3], "?"), "ISTACOX" = c(t1[3], k1), "ISTACOX ada" = c(t2[3], k2), "FISTACOX" = c(t3[3], k3), "FISTACOX ada" = c(t4[3], k4),
                         row.names=c("Temps","Nbre d'itérations"), check.names = F))

```

Les valeurs des coefficients obtenus sont très proches :

```{r tab_coefs, echo=FALSE, results='asis'}
knitr::kable(data.frame("COXNET" = as.vector(coef(coxnet_res, s="lambda.min")), "ISTACOX" = res1$beta, "ISTACOX ada" = res2$beta, "FISTACOX" = res3$beta, "FISTACOX ada" = res4$beta,
                        "$\\beta^*$" = beta_star,
                        check.names = F))
```




```{r plotbetas, echo=FALSE}
layout(matrix(1:4,2,2))
plot(beta_star, res1$beta, main=paste("ISTACOX cor=", round(cor(beta_star, res1$beta), digits = 2), sep=''), xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
plot(beta_star, res2$beta, main=paste("ISTACOX ada cor=", round(cor(beta_star, res2$beta), digits = 2), sep=''), xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
plot(beta_star, res3$beta, main=paste("FISTACOX cor=", round(cor(beta_star, res3$beta), digits = 2), sep=''), xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
plot(beta_star, res4$beta, main=paste("FISTACOX ada cor=", round(cor(beta_star, res4$beta), digits = 2), sep=''), xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
```

# Test de la fonction istacox_lambda_tune
Afin de sélectioner le paramètre lambda, on se sert de la fonction \verbatim{istacox_lambda_tune}, qui prend en paramètre une grille de lambda, pour lesquels il faut calculer les coefficients \beta. On a besoin au préalable des fonctions \verbatim{istacox.predict} et \verbatim{istacox.score}, entre autres annexes.
La métrique utilisée pour évaluer un modèle est la vraisemblance partielle parcimonieuse, comme dans l'article de Goeman et al. 2010. Elle est calculée par la fonction de prédicion et la fonction de scoring ne fait que renvoyer le résultat de la fonction de prédiction. Ceci pour préserver la généricité du framework MapRedR. Il sera envisagé plus tard de calculer d'autres métriques comme le score de Brier.

```{r}
norm.l2 <- function(beta){
  return(sqrt(sum(beta^2)))
}

sparse.partial.loglik <- function(model, newdata, newy, lambda){
  # model : beta of the fitted model
  # newdata : data from de test set
  # newy : outcome from de the test set to compute I and R
  
  beta <- model$beta
  ### Sets of patient at risk at Ti
  x.o <- newdata[order(newy[, 1]), ]
  y.o <- as.data.frame(newy[order(newy[, 1]), ])
  
  # uncensored patients
  I <- which(y.o$status==1)
  #print(I)
  # patients at risk
  R <- lapply( which(y.o$status==1) , function(i) which( y.o$time >= y.o$time[i] ) )
  names(R) <- paste0("R", which(y.o$status==1))
  #print(R)
  
  #pll <- mapply( function(i, j) sum(newdata[i, ]%*%beta - log(sum( exp(newdata[R[[sprintf("R%i", j)]], ]%*%beta) ))), I, R)
  pll <- sum(mapply( function(i) newdata[i, ]%*%beta - log(sum( exp(newdata[R[[sprintf("R%i", i)]], ]%*%beta) )), I))
  alpha <- 0.5*lambda # à justifier cf elasticNet
  gamma <- 0.25*lambda # à justifier cf elasticNet
  spll <- pll - (alpha*norm.l1(beta)) - (gamma*norm.l2(beta))
  
  return(spll=spll)
}

istacox.score <- function(y.test, y.hat, type="spll"){
  
  if(type=="spll"){
    #spll <- sparse.partial.loglik(model, newdata, newy, lambda)
    return(list(perf=y.hat))
  } else {
    print("Sorry, this type of score is not yet implemented !")
  }
}



istacox.predict <- function(model.train, x.test, y.test, lambda){
  #   model.train beta estimates from the train set, 
  #   x.test matrix of covariates of the test set, 
  #   y.test outcome of the test set, 
  
  library(survival)
  #source("/home/philippe/github/multiblox/istacox_MapRedR/scripts/sparse.partial.loglik.R")
  #source("/home/cathy/git_repo/multiblox/istacox_MapRedR/scripts/sparse.partial.loglik.R")
  
  pll <- sparse.partial.loglik(model=model.train, newdata=x.test, newy=y.test, lambda=lambda)
  
  return(list(est=pll))
  
}


istacox.lambda.tune <-
  function(X, y, trainmat, i, outer_it, lambda.grid, scale=T, method="CV", metric="spll", adaptative=TRUE, fast=TRUE){
    # x is a list of B (p_k by n) matrices of predictors
    # y dataframe of survival times and censoring, 
    # trainmat matrix of training sets indices
    # outer_it outerloop iteration
    # lambda.grid is a matrix containing all combinations of lambdas over blocks to test
    # adaptative : if true, compute an adaptative step for each ista iteration
    # TODO : propose several different metrics : inner_cv_metric=c("auc", "F", "wrec")
    
    cat("### outer CV : ", outer_it , ", inner CV iteration : ", i, "\n")
    
    library(MASS)
    library(CMA)
    library(survival)
  #  library(multiblox)
#   source("/home/philippe/github/multiblox/istacox_MapRedR/scripts/istacox.R")
#   source("/home/philippe/github/multiblox/istacox_MapRedR/scripts/istacox.predict.R")
#   source("/home/philippe/github/multiblox/istacox_MapRedR/scripts/istacox.score.R")
#   source("/home/philippe/github/multiblox/istacox_MapRedR/scripts/functions.R")
    
    B <- length(X) # nb of blocks
  if(B==1){
    N <- nrow(X[[1]]) # nb of individuals
  }else{
    N <- nrow(X) # nb of individuals
  }
    
    beta.train <- eta.train <- eta.test <- NULL
    pred.score <- res <- model <- cox.model <- NULL
    
    ### Beta initialization without any intercept
    beta0 <- NULL
    
    ##1) get train and test
    ind <- trainmat[i,]
    # indexing
    y.train <- y[ind,]
    y.test <- y[-ind,]
#     print(y.test)
#     X.train <- lapply(X, function(mm) mm[ind, ])
#     if (method=="LOOCV") {
#       X.test <- lapply(X, function(mm) t(as.matrix(mm[-ind,])))
#     } else {
#       X.test <- lapply(X, function(mm) { as.matrix(mm[-ind,])})
#     }
#     # scaling
#     if (scale) {
#       X.train <- lapply(X.train, function(mm) scale2(mm))
#       scl_fun <- function(data, scaled) {
#         scale(data, center = attr(scaled, "scaled:center"),
#               scale = attr(scaled, "scaled:scale")) }
#       X.test <- mapply(scl_fun, X.test, X.train, SIMPLIFY=FALSE)
#     } 

    ### pour un bloc
    X.train <- X[[1]][ind, ]
    if (method=="LOOCV") {
      X.test <- t(as.matrix(X[[1]][-ind,]))
    } else {
      X.test <- as.matrix(X[[1]][-ind,])
    }
    # scaling
    if (scale) {
      X.train <- scale2(X.train)
      scl_fun <- function(data, scaled) {
        scale(data, center = attr(scaled, "scaled:center"),
              scale = attr(scaled, "scaled:scale")) }
      X.test <- scl_fun(X.test, X.train)
    } 


    ### Sets of patient at risk at Ti
   
    x.o <- X.train[order(y.train[, 1]), ]
    
    y.o <- as.data.frame(y.train[order(y.train[, 1]), ])

    # uncensored patients
    I.train <- which(y.o$status==1)
    # patients at risk
    R.train <- lapply( which(y.o$status==1) , function(i) which( y.o$time >= y.o$time[i] ) )
    names(R.train) <- paste0("R", which(y.o$status==1))


    ### pathwise warm restart
### en monobloc, la grille de lambda n'est plus en deux dimensions...
    if(B==1){
      lambda.grid <- t(t(lambda.grid))
    }
    for (l in 1:nrow(lambda.grid)){
      cat("lambdas : ", l, " -> ")
      for (bid in 1:length(lambda.grid[l,])) {
        cat(" ", lambda.grid[l,bid], sep="")
      }
      cat("\n")
      
      ##2) Estimating the betas on the training set x, I, R, alpha, gamma, eps = 0.001, kmax = 10000
      print(adaptative)
      res[[l]]<- istacox(X=x.o, I.train, R.train, alpha=0.5*lambda.grid[l,], gamma=0.25*lambda.grid[l,], kmax=1000, ada=as.logical(adaptative), fast=as.logical(fast))
      beta.train <- res[[l]]$beta

      ### predict
      pred <- istacox.predict(model=res[[l]], x.test=X.test, y.test=y.test, lambda=lambda.grid[l,])
      
      ##5) get and accumulate the score
      pred.score[[l]] <- istacox.score(as.matrix(y.test), pred$est, type=metric)$perf
      beta0 <- res[[l]]$beta # update beta0 for warm restart
    }
    return(list(res=res, pred.score=pred.score))
  }

```

La grille des lambdas est produite par la fonction \verbatim{make.lambda.grid}.
```{r}

scale2<-function (A, center = TRUE, scale = TRUE) 
{
  if (center == TRUE & scale == TRUE) {
    A = scale(A, center = TRUE, scale = FALSE)
    std = apply(A, 2, sd)
    if (any(std==0)) {
      sprintf("there were %d constant variables",sum(std==0))
      std[std==0]=1
    }
    A = A/matrix(rep(std, nrow(A)), nrow(A), ncol(A), byrow = TRUE)
    attr(A, "scaled:scale") = std
    return(A)
  }
  if (center == TRUE & scale == FALSE) {
    A = scale(A, center = TRUE, scale = FALSE)
    return(A)
  }
  if (center == FALSE & scale == TRUE) {
    std = apply(A, 2, sd)
    A = A/matrix(rep(std, nrow(A)), nrow(A), ncol(A), byrow = TRUE)
    attr(A, "scaled:scale") = std
    return(A)
  }
}

make.lambda.grid <-
  function(X, path=c("naive", "smart", "norm1")){
    ##################################################################
    ###
    ###  Ridge Lambda grid definition cf Coxnet, Simon et al, JSS 2011.
    ###
    ##################################################################

    B <- length(X) # nb of blocks
    
    # scaling
    scale <- TRUE
    if (scale) {
      X <- lapply(X, function(mm) scale2(mm))
    }
    
    lambda.max <- rep(0, B)
    eps <- 0.005
    for (i in 1:B){
      lambda.max[[i]] <- 0.5*sum(apply(X[[i]], 2, function(c) norm(c, "2")))
    }
    lambda.min <- eps*lambda.max
    
    # -1 because sequence starts at 0 (0:nb.lambda) and -1 to include huge lambda (<Inf)
    nb.lambda <- 10 - 1 - 1 # corresponds to m, in the paper
    l <- NULL
    # 0 to generate keep space for huge lambda
    j <- c(0, 0:nb.lambda) 
    for(i in 1:B){
      l[[i]]<- lambda.max[[i]]*(lambda.min[[i]]/lambda.max[[i]])^(j/nb.lambda)
      # introduce huge lambda at the beginning
      l[[i]][[1]] <- 1e+3 # à partir de 1e+4 il y a une erreur dans la mise à jour de beta, ds la fct istacox.
    }
    if(B==1){
      print("un seul bloc !!")
      lg <- as.matrix(expand.grid(l))
      lambda.grid <- matrix(0, nrow=length(lg), ncol=1)
      lambda.grid[, 1] <- lg
    }else{
      lambda.grid <- as.matrix(expand.grid(l))
    }
    
    
    ### pathwise : naive ranking
    if (path == "naive") {
      n1 <- nrow(lambda.grid):1
    } else if (path  == "norm1") {
      n1 = apply(lambda.grid, 1, function(u) u[[1]] + u[[2]]) #seulement pour B=2
    } else if (path == "smart") {
      ## TODO ##
      n1 <- nrow(lambda.grid):1
    }
    #lambda.grid = lambda.grid[order(n1, decreasing=T),]
    return(lambda.grid[order(n1, decreasing=T),])
  }

make.grid <- function(data){
  return (make.lambda.grid(data, path="naive"))   
}

```
Fabriquons la grille pour nos données simulées :

```{r lambda grid, echo = FALSE}
Xl <- list()
Xl[[1]] <- X
grid <- make.grid(Xl)
```
 Il faut également fournir une matrice d'entrainement. Nous choisissons la validation croisée Monte Carlo (MCCV), en 5 plis.
```{r train matrix, echo = FALSE}
library("CMA")
set.seed(123456)
train.mat <- GenerateLearningsets(n=N, y = y[, "status"], method="MCCV", niter=5, ntrain=floor(N * 0.8) , strat=TRUE)@learnmatrix
attr(train.mat, "method") <- "MCCV"

```

 
 Et passons la grille et la matrice d'entrainement à la fonction \verbatim{istacox.lambda.tune} :
 
```{r istacox lambda tune, echo=FALSE}

X.ol <- list()
X.ol[[1]] <- X.o
res.lambda <- istacox.lambda.tune(X = X.ol, y = y.o, trainmat = train.mat, outer_it = 5, i = 5, lambda.grid = grid, scale=T, method="MCCV", metric="spll", adaptative=FALSE, fast=FALSE)

plot(res.lambda$pred.score~grid, xlab="lambdas", ylab="sparse partial loglikelihood", type="l")
cor.beta <- lapply(res.lambda$res, function(l){cor(l$beta, beta_star, use = "na")})
plot(cor.beta~grid, xlab="lambdas", ylab="beta* correlation")

```
