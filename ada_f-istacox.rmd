---
title: "ISTA et FISTA adaptatifs pour la régression de Cox"
author: "Vincent Guillemot et Cathy Philippe"
date: "27/07/2015"
output: pdf_document
header-includes: 
  \newcommand{\prox}{\text{prox}}
  \usepackage{graphicx}
---

Le problème de la régression de Cox  de $X$ sur $y$ avec une pénalité $\ell_1$ et une pénalité Ridge se formule de la manière suivante :

\[
  \min_{\beta} \left\{ - \ell(X,y,\beta) + \alpha \|\beta\|_1 + \gamma \|\beta\|_2^2 \right\},
\]
avec $\ell$ la fonction de vraisemblance partielle du modèle de Cox. La vraisemblance devant être maximisée, nous la faisons précéder du signe moins pour transformer le problème en un problème de minimisation.

# Principe

Comme pour la régression multiple, le principe de cette méthode est de calculer à chaque itération la mise à jour du vecteur des poids de la manière suivante :
\[
  \beta^{k+1} \leftarrow \prox_{t \lambda \|\cdot\|_1} \left(  \beta^k - t \nabla_\ell (\beta^k) \right),
\]
avec

* $\nabla_\ell (\beta) = ???$[^1],
* $t = ???$[^2],
* $\prox_{t \alpha \|\cdot\|_1}$ est l'opérateur proximal de la norm $\ell_1$ multiplié par $t \lambda$.

[^1]: cf formule dans le manuscrit de thèse.
[^2]: on ne sait pas déduire $t$ de manière explicite à partir de la constante de Lipschitz du gradient. On donne donc une valeur complètement arbitraire, et on va comparer avec une méthode adaptative. Normalement la méthode adaptative devrait ici montrer son intérêt !


# Implémentation en R

Nous proposons d'implémenter en R cet algorithme. Il faut tout d'abord déclarer les fonctions permettant de calculer le gradient de la fonction de perte et l'opérateur proximal.

```{r definitions}
grad <- function(X, beta, I, R, gamma) {
  wij <- mapply( function(i, j) t(exp( X[j, ]%*%beta) / sum( exp(X[R[[sprintf("R%i", i)]], ]%*%beta))), I, R)
  names(wij) <- names(R)
  xbar <- t(sapply(names(R), function(r) wij[[r]]%*%X[R[[r]], ]) )
  grad <- -colSums( X[I, ] - xbar ) + gamma*beta 
}

neg_step <- function(X, t, beta, alpha, g){
  return((1/t) * (beta - prox(beta - t * g, t, alpha)))
}

R_ell <- function(X, beta, I, R, gamma){
  truc <- mapply( function(i, j) {X[j, ]%*%beta -log(sum( exp(X[R[[sprintf("R%i", i)]], ]%*%beta)))}, I, R, SIMPLIFY = F)
  - sum(unlist(truc)) + gamma/2 * norm.l2.2(beta)
}

prox <- function(beta,t,alpha) ifelse(abs(beta)<t*alpha,0,beta-t*alpha*sign(beta))

norm.l1 <- function(beta){
  return(sum(abs(beta)))
}

norm.l2.2 <- function(beta){
  return(sum(beta^2))
}

```

Ces fonctions étant définies, nous pouvons implémenter les algorithmes ISTA et FISTA, avec un pas adaptatif ou non.

```{r ista}
istacox_step_line_search <- function(X, beta, I, R, t=10, tau=0.5, alpha, gamma, kmax=1000){
  R_l <- R_ell(X, beta, I, R, gamma)
  g <- grad(X, beta, I, R, gamma)
  Gt <- neg_step(X, t, beta, alpha, g)
  Rt_l <- R_ell(X, beta - t*Gt, I, R, gamma)

  for (k in 1:kmax) {
    if (Rt_l <= R_l -t*t(g) %*% Gt + t/2*sum(Gt**2)) break
    t <- tau*t
    Gt <- neg_step(X, t, beta, alpha, g)
    Rt_l <- R_ell(X, beta - t*Gt, I, R, gamma)
  }
  return(t)
}


fistacox_step_line_search <- function(X, u, I, R, t=10, tau=0.95, alpha, gamma, kmax=1000){
  Ru_l <- R_ell(X, u, I, R, gamma)
  gradu <- grad(X, u, I, R, gamma)
  x <- prox(u - t*gradu, t, alpha)
  Rx_l <- R_ell(X, x, I, R, gamma)

  for (k in 1:kmax) {
    if (Rx_l <= Ru_l +  crossprod(gradu, x-u) + 1/(2*t)*sum((x-u)**2)) break
    t <- tau*t
    x <- prox(u - t*gradu,t,alpha)
    Rx_l <- R_ell(X, x, I, R, gamma)
  }
  return(t)
}

istacox <- function(X, I, R, alpha, gamma, kmax=1000, epsilon=1e-10, 
                     fast=FALSE, ada=FALSE) {
  p <- ncol(X)
  n <- nrow(X)
  betaold <- rnorm(p)
  t <- 1/max(eigen(t(X)%*%X)$values)

  betanew <- prox(betaold - t*grad(X, betaold, I, R, gamma),t,alpha)

  for (k in 2:kmax) {
    if (fast) {
      u <- betanew + (k-1) / (k+2) * (betanew - betaold)
    } else {
      u <- betanew
    }
    if (ada) {
      if (fast) {
        t <- fistacox_step_line_search(X, u, I, R, t, tau=0.95, alpha, gamma)
      } else {
        t <- istacox_step_line_search(X, betaold, I, R, t, tau=0.95, alpha, gamma)
      }
    }
    betaold <- betanew
    betanew <- prox(u - t*grad(X, u, I, R, gamma), t, alpha)
    if (sum((betanew-betaold)**2) < epsilon ) break
  }
  return(list(beta=betanew, k=k))
}

```

# Test sur données simulées

Le script générant des données simulées présenté dans le cadre suivant est extrait de l'aide de la fonction `glmnet::glmnet` :

```{r coxsim, echo=TRUE}
#Cox
set.seed(10101)
N <- 250 ; p <- 66
nzc <- p/3
X <- matrix(rnorm(N*p),N,p)
betastar <- rnorm(nzc)
fx <- X[,seq(nzc)]%*%betastar/3
hx <- exp(fx)
ty <- rexp(N,hx)
# censoring indicator
tcens <- rbinom(n=N, prob=.3, size=1)
# y=Surv(ty,1-tcens) with library(survival)
y <- cbind(time=ty,status=1-tcens) 
```

L'application de la méthode `coxnet` à ce jeu de données simulées donne le résultat suivant (notamment sous forme graphique) :

```{r glmnet, fig.align="center", fig.cap="Graphe des coefficients en fonction du paramètre $\\alpha$."}
require(glmnet)
fit <- glmnet(X, y, family="cox")
plot(fit)
```

Avant de pouvoir appliquer ISTA, il faut ordonner les individus, calculer l'ensemble des patients censurés `I` et les ensembles de patients à risque `R`.

```{r i_et_r}
# Ordering data
X.o <- X[order(y[, 1]), ]
y.o <- as.data.frame(y[order(y[, 1]), ])

# uncensored patients
I <- which(y.o$status==1)
# patients at risk
R <- lapply( which(y.o$status==1) , function(i) which( y.o$time >= y.o$time[i] ) )
names(R) <- paste0("R", which(y.o$status==1))
```

Cela fait, on applique ISTA sur les données ordonnées.

```{r}
beta_star <- c(betastar/3, rep(0, p-nzc))
system.time(res <- istacox(X.o, I, R, alpha=10, gamma=10, kmax=1000, epsilon=1e-20) )
(res$k)
knitr::kable(df <- data.frame("ISTA COX" = res$beta, "$\\beta^*$" = beta_star, check.names=F))
```

## Test sur données simulées

Le test sur les mêmes données simulées que précedemment permet de comprendre l'intérêt que l'on a à connaître la valeur de la constante de Lipschitz $L$: cela permet d'économiser du temps de calcul...

```{r comparaison, echo=FALSE, results='asis'}
t0 <- system.time(coxnet_res <- cv.glmnet(x = X, y = y, type.measure = "deviance", family = "cox", alpha=0.5))
(lambda <- coxnet_res$lambda.min/10)
alpha1 <- 10 # lambda
gamma1 <- 10 # lambda
t1 <- system.time(res1 <- istacox(X.o, I, R, alpha=alpha1, gamma=gamma1, kmax=1000, fast = F, ada = F))
t2 <- system.time(res2 <- istacox(X.o, I, R, alpha=alpha1, gamma=gamma1, kmax=1000, fast = T, ada = F))
t3 <- system.time(res3 <- istacox(X.o, I, R, alpha=alpha1, gamma=gamma1, kmax=1000, fast = F, ada = T))
t4 <- system.time(res4 <- istacox(X.o, I, R, alpha=alpha1, gamma=gamma1, kmax=1000, fast = T, ada = T))

k1 <- res1$k
k2 <- res2$k
k3 <- res3$k
k4 <- res4$k

knitr::kable( data.frame("COXNET" = c(t0[3], "?"), "ISTACOX" = c(t1[3], k1), "ISTACOX ada" = c(t3[3], k2), "FISTACOX" = c(t2[3], k3), "FISTACOX ada" = c(t4[3], k4),
                         row.names=c("Temps","Nbre d'itérations"), check.names = F))

```

Les valeurs des coefficients obtenus sont très proches :

```{r tab_coefs, echo=FALSE, results='asis'}
knitr::kable(data.frame("COXNET" = as.vector(coef(coxnet_res, s="lambda.min")), "ISTACOX" = res1$beta, "ISTACOX ada" = res2$beta, "FISTACOX" = res3$beta, "FISTACOX ada" = res4$beta,
                        "$\\beta^*$" = beta_star,
                        check.names = F))
```




```{r plotbetas, echo=FALSE}
layout(matrix(1:4,2,2))
plot(beta_star, res1$beta, main="ISTACOX", xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
plot(beta_star, res2$beta, main="ISTACOX ada", xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
plot(beta_star, res3$beta, main="FISTACOX", xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
plot(beta_star, res4$beta, main="FISTACOX ada", xlab=expression(beta^symbol("*")), ylab=expression(hat(beta)))
```

